{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Title: Exercice 4.2\n",
    "##### Author: Jerock Kalala\n",
    "##### Date: January 20th 2023\n",
    "##### Modified By: --\n",
    "##### Using Natural Language Processing (NLP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. In the text, there’s a text normalizer created – your assignment is to re-create that normalizer as a Python class that can be re-used (within a .py file). However, unlike the book author’s version, pass a Pandas Series (e.g., dataframe[‘column’]) to your normalize_corpus function and use apply/lambda for each cleaning function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import  pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "class TextNormalizer:\n",
    "\n",
    "    def normalize_corpus(self, corpus: pd.Series, html_stripping=True, contraction_expansion=True, accented_char_removal=True,\n",
    "                     text_lower_case=True, text_lemmatization=True, special_char_removal=True,\n",
    "                     stopword_removal=True, remove_digits=True) -> pd.Series:\n",
    "        \"\"\"\n",
    "        corpus : pd.Series :  A pandas series containing text data\n",
    "        html_stripping : bool :  whether or not to remove html tags from the text\n",
    "        contraction_expansion : bool : whether or not to expand contractions\n",
    "        accented_char_removal : bool : whether or not to remove accented characters\n",
    "        text_lower_case : bool : whether or not to convert text to lowercase\n",
    "        text_lemmatization : bool : whether or not to lemmatize text\n",
    "        special_char_removal : bool : whether or not to remove special characters and/or digits\n",
    "        stopword_removal : bool : whether or not to remove stopwords\n",
    "        remove_digits : bool : whether or not to remove digits from the text\n",
    "        \"\"\"\n",
    "        corpus = corpus.apply(lambda doc: self.strip_html_tags(doc) if html_stripping else doc)\n",
    "        corpus = corpus.apply(lambda doc: self.remove_accented_chars(doc) if accented_char_removal else doc)\n",
    "        corpus = corpus.apply(lambda doc: self.expand_contractions(doc) if contraction_expansion else doc)\n",
    "        corpus = corpus.apply(lambda doc: doc.lower() if text_lower_case else doc)\n",
    "        corpus = corpus.apply(lambda doc: re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc))\n",
    "        corpus = corpus.apply(lambda doc: self.text_lemmatization(doc) if text_lemmatization else doc)\n",
    "        corpus = corpus.apply(lambda doc: self.remove_special_character(doc, remove_digits) if special_char_removal else doc)\n",
    "        corpus = corpus.apply(lambda doc: re.sub(' +', ' ', doc))\n",
    "        corpus = corpus.apply(lambda doc: self.remove_stopwords(doc, is_lower_case=text_lower_case) if stopword_removal else doc)\n",
    "        return corpus\n",
    "\n",
    "    def strip_html_tags(self, text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "        return stripped_text\n",
    "    def remove_accented_chars(self, text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    def text_lemmatization(self, text):\n",
    "        nlp = spacy.load('en_core', parse=True, tag=True, entity=True )\n",
    "        text = nlp(text)\n",
    "        text = ' '.join([word.lemma_ if word.lemma_ !='-PRON-' else word.text for word in text])\n",
    "        return text\n",
    "\n",
    "    def remove_special_character(self, text, remove_digits=False):\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text, is_lower_case=False):\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "        stopword_list = set(stopword_list)\n",
    "        if is_lower_case:\n",
    "            text = [word for word in text.split() if word.lower() not in stopword_list]\n",
    "        else:\n",
    "            text = [word for word in text.split() if word not in stopword_list]\n",
    "        return ' '.join(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Using your new text normalizer, create a Jupyter Notebook that uses this class to clean up the text found in the file big.txt (that text file is in the GitHub for Week 4 repository). Your resulting text should be a (long) single stream of text."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 13, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# read the contents of big.txt into a pandas series\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m big_txt \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbig.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# create an instance of the TextNormalizer class\u001B[39;00m\n\u001B[0;32m      5\u001B[0m normalizer \u001B[38;5;241m=\u001B[39m TextNormalizer()\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    312\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    313\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    315\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(inspect\u001B[38;5;241m.\u001B[39mcurrentframe()),\n\u001B[0;32m    316\u001B[0m     )\n\u001B[1;32m--> 317\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    610\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1772\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1765\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1767\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1768\u001B[0m     (\n\u001B[0;32m   1769\u001B[0m         index,\n\u001B[0;32m   1770\u001B[0m         columns,\n\u001B[0;32m   1771\u001B[0m         col_dict,\n\u001B[1;32m-> 1772\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1773\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1774\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1775\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1776\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:243\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 243\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    245\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 1 fields in line 13, saw 5\n"
     ]
    }
   ],
   "source": [
    "# read the contents of big.txt into a pandas series\n",
    "big_txt = pd.read_csv('big.txt', header=None, names=['text'])\n",
    "\n",
    "# create an instance of the TextNormalizer class\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "# normalize the big_txt series using the normalize_corpus function\n",
    "big_txt_normalized = normalizer.normalize_corpus(big_txt['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: I couldn't get the above error fixed. I have emailed you for help as my professor with the knowledge and experience you have in python coding but didn't get the needed help. instead, you asked me to get it submitted before the EOD. Not being able to solve it on my own, I submitted it as is but got the text normalized by passing the text to each function and saving the normalized text into a file named big_txt_normalized.txt.  I still hope to receive assistance to fix that error, because I don't only need a good grade but knowledge is the most important."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "big_txt_normalized = Path('big_txt_normalized.txt').read_text()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Using spaCy and NLTK, show the tokens, lemmas, parts of speech, and dependencies in the first 1,021 characters of big.txt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the DET det\n",
      "Project Project PROPN compound\n",
      "Gutenberg Gutenberg PROPN compound\n",
      "EBook EBook PROPN nsubj\n",
      "of of ADP prep\n",
      "the the DET det\n",
      "Adventures Adventures PROPN pobj\n",
      "of of ADP prep\n",
      "Sherlock Sherlock PROPN compound\n",
      "Holmesby Holmesby PROPN compound\n",
      "Sir Sir PROPN compound\n",
      "Arthur Arthur PROPN compound\n",
      "Conan Conan PROPN compound\n",
      "Doyle15 Doyle15 PROPN pobj\n",
      "in in ADP prep\n",
      "our our PRON poss\n",
      "series series NOUN pobj\n",
      "by by ADP prep\n",
      "Sir Sir PROPN compound\n",
      "Arthur Arthur PROPN compound\n",
      "Conan Conan PROPN compound\n",
      "doylecopyright doylecopyright PROPN compound\n",
      "law law NOUN pobj\n",
      "be be AUX aux\n",
      "change change NOUN ROOT\n",
      "all all ADV advmod\n",
      "over over ADP prep\n",
      "the the DET det\n",
      "world world NOUN pobj\n",
      "    SPACE dep\n",
      "be be AUX advcl\n",
      "sure sure ADJ acomp\n",
      "to to PART aux\n",
      "check check VERB xcomp\n",
      "thecopyright thecopyright ADJ amod\n",
      "law law NOUN dobj\n",
      "for for ADP prep\n",
      "your your PRON poss\n",
      "country country NOUN pobj\n",
      "before before ADP prep\n",
      "download download NOUN pobj\n",
      "or or CCONJ cc\n",
      "redistributingthis redistributingthis NOUN conj\n",
      "or or CCONJ cc\n",
      "any any DET det\n",
      "other other ADJ amod\n",
      "Project Project PROPN compound\n",
      "Gutenberg Gutenberg PROPN compound\n",
      "eBook eBook PROPN conj\n",
      "    SPACE dep\n",
      "this this DET det\n",
      "header header NOUN nsubj\n",
      "should should AUX aux\n",
      "be be AUX conj\n",
      "the the DET det\n",
      "first first ADJ amod\n",
      "thing thing NOUN nsubj\n",
      "see see VERB acl\n",
      "when when SCONJ advmod\n",
      "view view VERB advcl\n",
      "this this DET det\n",
      "ProjectGutenberg ProjectGutenberg PROPN compound\n",
      "file file NOUN nsubj\n",
      "        SPACE dep\n",
      "please please INTJ intj\n",
      "do do AUX aux\n",
      "not not PART neg\n",
      "remove remove VERB ccomp\n",
      "it it PRON dobj\n",
      "        SPACE dep\n",
      "do do AUX aux\n",
      "not not PART neg\n",
      "change change VERB conj\n",
      "or or CCONJ cc\n",
      "edit edit VERB conj\n",
      "theheader theheader NOUN dobj\n",
      "without without ADP prep\n",
      "write write ADJ amod\n",
      "permission permission NOUN pobj\n",
      "    SPACE dep\n",
      "please please INTJ intj\n",
      "read read VERB conj\n",
      "the the DET det\n",
      "    SPACE dep\n",
      "legal legal ADJ amod\n",
      "small small ADJ amod\n",
      "print print NOUN dobj\n",
      "      SPACE dep\n",
      "and and CCONJ cc\n",
      "other other ADJ amod\n",
      "information information NOUN conj\n",
      "about about ADP prep\n",
      "theebook theebook NOUN pobj\n",
      "and and CCONJ cc\n",
      "Project Project PROPN compound\n",
      "Gutenberg Gutenberg PROPN conj\n",
      "at at ADP prep\n",
      "the the DET det\n",
      "bottom bottom NOUN pobj\n",
      "of of ADP prep\n",
      "this this DET det\n",
      "file file NOUN pobj\n",
      "        SPACE dep\n",
      "include include VERB conj\n",
      "isimportant isimportant ADJ amod\n",
      "information information NOUN dobj\n",
      "about about ADP prep\n",
      "your your PRON poss\n",
      "specific specific ADJ amod\n",
      "right right NOUN pobj\n",
      "and and CCONJ cc\n",
      "restriction restriction PROPN conj\n",
      "inhow inhow PROPN conj\n",
      "the the DET det\n",
      "file file NOUN nsubjpass\n",
      "may may AUX aux\n",
      "be be AUX auxpass\n",
      "use use VERB relcl\n",
      "        SPACE dep\n",
      "you you PRON nsubj\n",
      "can can AUX aux\n",
      "also also ADV advmod\n",
      "find find VERB conj\n",
      "out out ADP prt\n",
      "about about ADP prep\n",
      "how how SCONJ advmod\n",
      "to to PART aux\n",
      "make make VERB pcomp\n",
      "adonation adonation NOUN dobj\n",
      "to to ADP prep\n",
      "Project Project PROPN compound\n",
      "Gutenberg Gutenberg PROPN pobj\n",
      "    SPACE dep\n",
      "and and CCONJ cc\n",
      "how how SCONJ advmod\n",
      "to to PART aux\n",
      "get get VERB conj\n",
      "involvedwelcome involvedwelcome ADJ acomp\n",
      "to to ADP prep\n",
      "the the DET det\n",
      "World World PROPN pobj\n",
      "of of ADP prep\n",
      "Free Free PROPN compound\n",
      "Plain Plain PROPN pobj\n",
      "Vanilla Vanilla PROPN compound\n",
      "Electronic Electronic PROPN compound\n",
      "textsebook textsebook NOUN pobj\n",
      "readable readable ADJ advcl\n",
      "by by ADP prep\n",
      "both both CCONJ preconj\n",
      "human human NOUN pobj\n",
      "and and CCONJ cc\n",
      "by by ADP conj\n",
      "Computers computer NOUN pobj\n",
      "    SPACE dep\n",
      "since since SCONJ mark\n",
      "1971These 1971These PROPN compound\n",
      "eBooks eBooks PROPN nsubjpass\n",
      "be be AUX auxpass\n",
      "prepared prepare VERB advcl\n",
      "by by ADP agent\n",
      "thousand thousand NUM pobj\n",
      "of of ADP prep\n",
      "volunteerstitle volunteerstitle NOUN pobj\n",
      "    SPACE dep\n",
      "the the DET det\n",
      "adventure adventure NOUN dobj\n",
      "of of ADP prep\n",
      "Sherlock Sherlock PROPN compound\n",
      "HolmesAuthor HolmesAuthor PROPN compound\n",
      "    SPACE dep\n",
      "Sir Sir PROPN compound\n",
      "Arthur Arthur PROPN compound\n",
      "Cona Cona PROPN pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# get the first 1021 characters of big_txt_normalized\n",
    "text = big_txt_normalized[:1021]\n",
    "\n",
    "# process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# print the tokens, lemmas, POS tags, and dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the DT\n",
      "Project Project NNP\n",
      "Gutenberg Gutenberg NNP\n",
      "EBook EBook NNP\n",
      "of of IN\n",
      "the the DT\n",
      "Adventures Adventures NNP\n",
      "of of IN\n",
      "Sherlock Sherlock NNP\n",
      "Holmesby Holmesby NNP\n",
      "Sir Sir NNP\n",
      "Arthur Arthur NNP\n",
      "Conan Conan NNP\n",
      "Doyle15 Doyle15 NNP\n",
      "in in IN\n",
      "our our PRP$\n",
      "series series NN\n",
      "by by IN\n",
      "Sir Sir NNP\n",
      "Arthur Arthur NNP\n",
      "Conan Conan NNP\n",
      "doylecopyright doylecopyright VBD\n",
      "law law NN\n",
      "be be VB\n",
      "change change VBN\n",
      "all all DT\n",
      "over over IN\n",
      "the the DT\n",
      "world world NN\n",
      "be be VB\n",
      "sure sure JJ\n",
      "to to TO\n",
      "check check VB\n",
      "thecopyright thecopyright JJ\n",
      "law law NN\n",
      "for for IN\n",
      "your your PRP$\n",
      "country country NN\n",
      "before before IN\n",
      "download download NN\n",
      "or or CC\n",
      "redistributingthis redistributingthis NN\n",
      "or or CC\n",
      "any any DT\n",
      "other other JJ\n",
      "Project Project NNP\n",
      "Gutenberg Gutenberg NNP\n",
      "eBook eBook VB\n",
      "this this DT\n",
      "header header NN\n",
      "should should MD\n",
      "be be VB\n",
      "the the DT\n",
      "first first JJ\n",
      "thing thing NN\n",
      "see see NN\n",
      "when when WRB\n",
      "view view NN\n",
      "this this DT\n",
      "ProjectGutenberg ProjectGutenberg NNP\n",
      "file file NN\n",
      "please please NN\n",
      "do do VB\n",
      "not not RB\n",
      "remove remove VB\n",
      "it it PRP\n",
      "do do VB\n",
      "not not RB\n",
      "change change VB\n",
      "or or CC\n",
      "edit edit VB\n",
      "theheader theheader NN\n",
      "without without IN\n",
      "write write JJ\n",
      "permission permission NN\n",
      "please please NN\n",
      "read read VB\n",
      "the the DT\n",
      "legal legal JJ\n",
      "small small JJ\n",
      "print print NN\n",
      "and and CC\n",
      "other other JJ\n",
      "information information NN\n",
      "about about IN\n",
      "theebook theebook NN\n",
      "and and CC\n",
      "Project Project NNP\n",
      "Gutenberg Gutenberg NNP\n",
      "at at IN\n",
      "the the DT\n",
      "bottom bottom NN\n",
      "of of IN\n",
      "this this DT\n",
      "file file NN\n",
      "include include VBP\n",
      "isimportant isimportant JJ\n",
      "information information NN\n",
      "about about IN\n",
      "your your PRP$\n",
      "specific specific JJ\n",
      "right right NN\n",
      "and and CC\n",
      "restriction restriction NN\n",
      "inhow inhow VBP\n",
      "the the DT\n",
      "file file NN\n",
      "may may MD\n",
      "be be VB\n",
      "use use IN\n",
      "you you PRP\n",
      "can can MD\n",
      "also also RB\n",
      "find find VB\n",
      "out out RP\n",
      "about about IN\n",
      "how how WRB\n",
      "to to TO\n",
      "make make VB\n",
      "adonation adonation NN\n",
      "to to TO\n",
      "Project Project NNP\n",
      "Gutenberg Gutenberg NNP\n",
      "and and CC\n",
      "how how WRB\n",
      "to to TO\n",
      "get get VB\n",
      "involvedwelcome involvedwelcome JJ\n",
      "to to TO\n",
      "the the DT\n",
      "World World NNP\n",
      "of of IN\n",
      "Free Free NNP\n",
      "Plain Plain NNP\n",
      "Vanilla Vanilla NNP\n",
      "Electronic Electronic NNP\n",
      "textsebook textsebook NN\n",
      "readable readable NN\n",
      "by by IN\n",
      "both both DT\n",
      "human human JJ\n",
      "and and CC\n",
      "by by IN\n",
      "Computers Computers NNP\n",
      "since since IN\n",
      "1971These 1971These CD\n",
      "eBooks eBooks NNS\n",
      "be be VB\n",
      "prepared prepared VBN\n",
      "by by IN\n",
      "thousand thousand NN\n",
      "of of IN\n",
      "volunteerstitle volunteerstitle NN\n",
      "the the DT\n",
      "adventure adventure NN\n",
      "of of IN\n",
      "Sherlock Sherlock NNP\n",
      "HolmesAuthor HolmesAuthor NNP\n",
      "Sir Sir NNP\n",
      "Arthur Arthur NNP\n",
      "Cona Cona NNP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jeroc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Done using NLTK:\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# get the first 1021 characters of big_txt_normalized\n",
    "text = big_txt_normalized[:1021]\n",
    "\n",
    "# tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# get the POS tags and lemmatize the tokens\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token in tagged_tokens:\n",
    "    print(token[0], lemmatizer.lemmatize(token[0]), token[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}