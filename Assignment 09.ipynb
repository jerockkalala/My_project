{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clustering with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assignment 9\n",
    "\n",
    "The following code initializes a Spark session that you use for the remainder of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DSC 400 Assignment 9\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# TODO: Change these to point to versions on your local path\n",
    "\n",
    "sample_kmeans_data_path = 'sample_kmeans_data.txt'\n",
    "sample_lda_data_path = 'sample_lda_libsvm_data.txt'\n",
    "sample_movielens_data_path = 'sample_movielens_ratings.txt'\n",
    "\n",
    "#or load the file using the path\n",
    "#sample_movielens_data_path = Path('sample_movielens_ratings.txt')\n",
    "#sample_movielens_data_path.exists() #check if the file exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 9.1\n",
    "\n",
    "Run the PySpark version of the k-means clustering example found in [Apache Spark's k-means clustering documentation](https://spark.apache.org/docs/latest/ml-clustering.html#k-means). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_kmeans_data.txt` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_kmeans_data.txt. Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 06:42:51 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(sample_kmeans_data_path)\n",
    "\n",
    "#if the file was loaded using the path lib, then the above code should look as below\n",
    "#dataset = spark.read.format(\"libsvm\").load(str(sample_kmeans_data_path)) #converted the file into a string\n",
    "\n",
    "# TODO: Implement the remainder of the code from the k-means clustering example\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 9.2\n",
    "\n",
    "Run the PySpark version of the Latent Dirichlet Allocation (LDA) example found in [Apache Spark's LDA documentation](https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_lda_libsvm_data.txt\n",
    "` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_lda_libsvm_data.txt\n",
    ". Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "dataset = dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_lda_libsvm_data.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 06:45:46 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "23/03/03 06:45:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/03/03 06:45:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "The lower bound on the log likelihood of the entire corpus: -795.0318110468672\n",
      "The upper bound on perplexity: 3.0578146578725662\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[9, 1, 4]  |[0.10398261025612052, 0.10189617618739767, 0.09803741465748232]|\n",
      "|1    |[10, 5, 1] |[0.10069698911487907, 0.09564812994560642, 0.09364296554449172]|\n",
      "|2    |[10, 2, 0] |[0.10202862302009798, 0.10116710279994974, 0.0974282863530427] |\n",
      "|3    |[0, 7, 8]  |[0.10035573873205858, 0.09808825787546796, 0.09724914962984825]|\n",
      "|4    |[2, 6, 7]  |[0.10993115939620668, 0.09624678603997004, 0.09418856285572906]|\n",
      "|5    |[9, 8, 2]  |[0.11250322785201246, 0.11052015335075367, 0.09702046828131099]|\n",
      "|6    |[1, 9, 2]  |[0.10697636323299374, 0.10142469477450025, 0.09619156434160409]|\n",
      "|7    |[2, 7, 3]  |[0.10359487333163302, 0.10218454592760035, 0.09779382096215898]|\n",
      "|8    |[6, 3, 5]  |[0.14959602283668516, 0.12349106630002316, 0.12055896639649685]|\n",
      "|9    |[10, 7, 3] |[0.10842171295606826, 0.09942251579630033, 0.09628270339455605]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                       |\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004629660208542292,0.004629685411312186,0.004629748089201779,0.0046296685386043885,0.31802808584711634,0.004629649794248323,0.004629680277354763,0.004629744568443692,0.6449343754996515,0.004629701765524818]       |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007728470141115921,0.007728446364511542,0.0077283794878495096,0.007728417787523394,0.007728419554342056,0.007728284922387317,0.007728374938362006,0.0077284537752585186,0.9304442583903471,0.007728494638302752]     |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004024319567967794,0.00402431367016838,0.0040243254307054405,0.004024315778419352,0.004024353507580127,0.0040243290476781185,0.004024344724370863,0.004024321267066896,0.9637810707057629,0.0040243063002802434]     |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003559126560116547,0.0035591605511998492,0.0035591568259565713,0.0035591189237655398,0.0035591463449078124,0.003559141683698236,0.0035591023053912043,0.003559149104952353,0.967967702347013,0.003559195352999003]   |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.0038563175788944993,0.0038563273769272985,0.0038563471419600344,0.0038563071544266128,0.0038564498975338576,0.003856319922961197,0.00385629213746192,0.0038563483135661894,0.9652929372244134,0.003856353251854948]  |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0035591502170005756,0.0035591239380768163,0.00355911057481935,0.0035591326726295666,0.0035591046244017846,0.0035591282259687567,0.0035591085544038996,0.003559096825296871,0.9679679304349366,0.0035591139324657995] |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0037017666053333854,0.0037018064437043894,0.003701802402495353,0.0037017624519095006,0.0037017928661218424,0.0037017811975991813,0.0037017413591709882,0.003701796948636284,0.9666838913573145,0.0037018583677146705]|\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004207666969193707,0.004207679280115989,0.004207700126185542,0.0042076513183163695,0.004207749311680815,0.00420766865918854,0.004207645218233025,0.004207702283158277,0.9621308340713417,0.004207702762585995]       |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004207705884452984,0.004207684449413309,0.004207661852240827,0.004207698303532382,0.0042076848675108875,0.004207618470103032,0.004207658624621954,0.00420768492406343,0.9621309000845494,0.004207702539511647]       |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.003190393808055393,0.0031903851397872838,0.00319038658183119,0.0031903695750247086,0.00319039833778492,0.0031904083565010666,0.003190396630907401,0.0031903798085516698,0.9712865066567311,0.0031903751048252865]    |\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004024334944416306,0.004024343877818726,0.004024364427095029,0.004024316536074439,0.004024383908515657,0.004024344954725758,0.004024321693546704,0.0040243543024827905,0.9637808800092194,0.004024355346105286]      |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.00462951007686269,0.004629496071803995,0.004629472919134325,0.0046297056839847365,0.004629479015095568,0.00462941586780561,0.004629462175452391,0.004629476873777251,0.9583344836401563,0.004629497675927143]        |\n",
      "+-----+---------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(sample_lda_data_path)\n",
    "\n",
    "# TODO: Implement the remainder of the code from the LDA example\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 9.3\n",
    "\n",
    "Run the PySpark version of the collaborative filtering example found in [Apache Spark's collaborative filtering documentation](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html). You can also find the code in [Apache Spark's Github repository](https://github.com/apache/spark/tree/master/examples/src/main/python/mllib). \n",
    "\n",
    "The example references a `sample_movielens_ratings.txt` file. You can find the file at https://raw.githubusercontent.com/apache/spark/master/data/mllib/als/sample_movielens_ratings.txt. Put this file in your working path and change the following code to point to your local version of the file. \n",
    "\n",
    "```\n",
    "lines = spark.read.text(\"data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.6304101084761125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lines = spark.read.text(sample_movielens_data_path).rdd\n",
    "\n",
    "# TODO: Implement the remainder of the code from the collaborative filtering example\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
